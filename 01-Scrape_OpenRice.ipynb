{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The following code is for getting all the restaurant url on openrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import uuid\n",
    "from datetime import date, datetime\n",
    "\n",
    "from selenium import webdriver\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import stem.process\n",
    "import json\n",
    "from fake_useragent import UserAgent\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select, WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "today = str(date.today()).replace(\"-\", \"\")\n",
    "\n",
    "\"\"\"--------------------------------------------------------Setting Up TOR ----------------------------------------------------------------\"\"\"\n",
    "\n",
    "SOCKS_PORT = 9050\n",
    "TOR_PATH = os.path.normpath(r\"C:\\Program Files\\Tor-master\\tor\\tor.exe\")\n",
    "tor_process = stem.process.launch_tor_with_config(\n",
    "    config={\n",
    "        \"SocksPort\": str(SOCKS_PORT),\n",
    "        \"MaxCircuitDirtiness\": \"1\",\n",
    "    },\n",
    "    init_msg_handler=lambda line: (\n",
    "        print(line) if re.search(\"Bootstrapped\", line) else False\n",
    "    ),\n",
    "    tor_cmd=TOR_PATH,\n",
    ")\n",
    "\n",
    "PROXIES = {\"http\": \"socks5://127.0.0.1:9050\", \"https\": \"socks5://127.0.0.1:9050\"}\n",
    "\n",
    "url = \"http://ip-api.com/json/\"\n",
    "ua = UserAgent()\n",
    "header = ua.random\n",
    "headers = {\"User-Agent\": header}\n",
    "response = requests.get(url, proxies=PROXIES, headers=headers)\n",
    "result = json.loads(response.content)\n",
    "print(\n",
    "    \"TOR IP [%s]: %s %s\"\n",
    "    % (datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"), result[\"query\"], result[\"country\"])\n",
    ")\n",
    "\n",
    "# tor_process.kill()\n",
    "\n",
    "\"\"\"----------------------------------------------------------------------------------------------------------------------\"\"\"\n",
    "\n",
    "df_district = pd.read_csv(\n",
    "    r\"C:\\Users\\crmhk\\Desktop\\Henry\\OpenRice\\OpenRice_All_Districts.csv\"\n",
    ")\n",
    "\n",
    "district_list = df_district[\"District\"].tolist()\n",
    "\n",
    "len(district_list)\n",
    "\n",
    "PROXIES = \"socks5://127.0.0.1:9050\"\n",
    "\n",
    "options = Options()\n",
    "# options.add_argument(\"--headless=new\")\n",
    "options.add_argument(f\"--proxy-server={PROXIES}\")\n",
    "link_prefix = \"https://www.openrice.com/\"\n",
    "\n",
    "# Now create a blank dataframe with column hearders of district_link, restaurant_link, and district and each begin with an empty list\n",
    "\n",
    "\n",
    "def scrape_resto_links_by_area(district_list):\n",
    "\n",
    "    df_restaurant_link = pd.DataFrame(\n",
    "        {\"district_link\": [], \"restaurant_link\": [], \"district\": []}\n",
    "    )\n",
    "\n",
    "    district_link = []\n",
    "    restaurant_name_list = []\n",
    "    restaurant_link = []\n",
    "    district_name = []\n",
    "\n",
    "    for district in district_list[0:]:\n",
    "        url = rf\"https://www.openrice.com/zh/hongkong/restaurants/district/{district}?sortBy=ORScoreDesc\"\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(url)\n",
    "        time.sleep(5)\n",
    "        # click_button = driver.find_element(\n",
    "        #     By.CLASS_NAME, \"cookies-agreement-btn filled-button or-theme-y\"\n",
    "        # )\n",
    "        # click_button.click()\n",
    "        window_size = driver.get_window_size()\n",
    "        window_width = window_size[\"width\"]\n",
    "        window_height = window_size[\"height\"]\n",
    "        prev_height = -1\n",
    "        max_scrolls = 10\n",
    "        scroll_count = 0\n",
    "\n",
    "        def get_safe_random_offset(max_value):\n",
    "            return random.randint(0, max_value - 1)\n",
    "\n",
    "        offset_x = get_safe_random_offset(133)\n",
    "        offset_y = get_safe_random_offset(133)\n",
    "\n",
    "        while scroll_count < max_scrolls:\n",
    "            # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            new_height = (\n",
    "                driver.execute_script(\"return document.body.scrollHeight\") - 1000\n",
    "            )\n",
    "            # print(height_to_scroll)\n",
    "            driver.execute_script(f\"window.scrollTo(0, {new_height});\")\n",
    "            # After scrolling down to the bottom of the page, randonly move the mouse to a different location to avoid detection\n",
    "            time.sleep(random.uniform(3, 5))\n",
    "            try:\n",
    "                action = webdriver.ActionChains(driver)\n",
    "                action.move_by_offset(offset_x, offset_y).perform()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                time.sleep(5)  # give some time for new results to load\n",
    "                # new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                if new_height == prev_height:\n",
    "                    break\n",
    "                prev_height = new_height\n",
    "                scroll_count += 1\n",
    "            except:\n",
    "                print(\n",
    "                    \"----------You have scrolled to the bottom of the page!-----------\"\n",
    "                )\n",
    "                break\n",
    "\n",
    "        # Fetch the data using BeautifulSoup after all data is loaded\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        all_restaurants = soup.find_all(\"div\", class_=\"poi-list-cell-desktop-right-top\")\n",
    "        for restaurant in all_restaurants:\n",
    "            link = link_prefix + restaurant.find(\"a\").get(\"href\")\n",
    "            restaurant_name = restaurant.find(\n",
    "                \"div\", class_=\"poi-name poi-list-cell-link\"\n",
    "            ).text.strip()\n",
    "            district_link.append(url)\n",
    "            restaurant_link.append(link)\n",
    "            district_name.append(district)\n",
    "            restaurant_name_list.append(restaurant_name)\n",
    "\n",
    "        driver.quit\n",
    "        time.sleep(3)\n",
    "\n",
    "    df_restaurant_link[\"district_link\"] = district_link\n",
    "    df_restaurant_link[\"restaurant_link\"] = restaurant_link\n",
    "    df_restaurant_link[\"district\"] = district_name\n",
    "    df_restaurant_link[\"restaurant_name\"] = restaurant_name_list\n",
    "\n",
    "    print(\"Done\")\n",
    "\n",
    "    return df_restaurant_link\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The following code is for getting to basic demographic information of all the restaurants in the openrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "from datetime import date, datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import stem.process\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from fake_useragent import UserAgent\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select, WebDriverWait\n",
    "\n",
    "today = str(date.today()).replace(\"-\", \"\")\n",
    "\n",
    "\"\"\"--------------------------------------------------------Setting Up TOR ----------------------------------------------------------------\"\"\"\n",
    "\n",
    "SOCKS_PORT = 9050\n",
    "TOR_PATH = os.path.normpath(r\"<Tor Path>\")\n",
    "tor_process = stem.process.launch_tor_with_config(\n",
    "    config={\n",
    "        \"SocksPort\": str(SOCKS_PORT),\n",
    "        \"MaxCircuitDirtiness\": \"1\",\n",
    "    },\n",
    "    init_msg_handler=lambda line: (\n",
    "        print(line) if re.search(\"Bootstrapped\", line) else False\n",
    "    ),\n",
    "    tor_cmd=TOR_PATH,\n",
    ")\n",
    "\n",
    "PROXIES = {\"http\": \"socks5://127.0.0.1:9050\", \"https\": \"socks5://127.0.0.1:9050\"}\n",
    "\n",
    "url = \"http://ip-api.com/json/\"\n",
    "ua = UserAgent()\n",
    "header = ua.random\n",
    "headers = {\"User-Agent\": header}\n",
    "response = requests.get(url, proxies=PROXIES, headers=headers)\n",
    "result = json.loads(response.content)\n",
    "print(\n",
    "    \"TOR IP [%s]: %s %s\"\n",
    "    % (datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"), result[\"query\"], result[\"country\"])\n",
    ")\n",
    "\n",
    "# tor_process.kill()\n",
    "\n",
    "\"\"\"----------------------------------------------------------------------------------------------------------------------\"\"\"\n",
    "\n",
    "scraping_status = []\n",
    "restaurant_url = []\n",
    "restaurant_name = []\n",
    "restaurant_being_saved_times = []\n",
    "restaurant_district = []\n",
    "restaurant_address = []\n",
    "restaurant_mtr_exit_or_mins_walk = []\n",
    "restaurant_branch_num = []\n",
    "restaurant_opening_hours = []\n",
    "restaurant_payment_method = []\n",
    "scraped_num_restaurants = 0\n",
    "\n",
    "df_restaurant_link = pd.read_json(rf\"<9000 restaurant link path>\")\n",
    "\n",
    "for restaurant_link in list(df_restaurant_link[\"restaurant_link\"])[0:]:\n",
    "\n",
    "    scraped_num_restaurants += 1\n",
    "\n",
    "    url = str(restaurant_link)\n",
    "\n",
    "    PROXIES = \"socks5://127.0.0.1:9050\"\n",
    "\n",
    "    def get_safe_random_offset(max_value):\n",
    "        return random.randint(0, max_value - 1)\n",
    "\n",
    "    link_prefix = \"https://www.openrice.com/\"\n",
    "    offset_x = get_safe_random_offset(133)\n",
    "    offset_y = get_safe_random_offset(133)\n",
    "\n",
    "    options = Options()\n",
    "    # options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(f\"--proxy-server={PROXIES}\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(random.uniform(4, 8))\n",
    "        driver.execute_script(f\"window.scrollBy(0, 800);\")\n",
    "        element = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            \".or-section-button.collapsed[data-toggle='collapse'][data-target='#pois-filter-expandable-features']\",\n",
    "        )\n",
    "        element.click()\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        soup.find(\"h1\").find(\"span\", class_=\"name\").text\n",
    "    except:\n",
    "        scraping_status.append(\"Failed to scrape, need to rescrape the url\")\n",
    "        restaurant_url.append(url)\n",
    "        restaurant_name.append(\"N/A\")\n",
    "        restaurant_being_saved_times.append(\"N/A\")\n",
    "        restaurant_district.append(\"N/A\")\n",
    "        restaurant_address.append(\"N/A\")\n",
    "        restaurant_mtr_exit_or_mins_walk.append(\"N/A\")\n",
    "        restaurant_branch_num.append(\"N/A\")\n",
    "        restaurant_opening_hours.append(\"N/A\")\n",
    "        restaurant_payment_method.append(\"N/A\")\n",
    "\n",
    "    else:\n",
    "        scraping_status.append(\"Scraped successfully\")\n",
    "        restaurant_url.append(url)\n",
    "        restaurant_name.append(soup.find(\"h1\").find(\"span\", class_=\"name\").text)\n",
    "        try:\n",
    "            restaurant_being_saved_times.append(\n",
    "                soup.find(\n",
    "                    \"div\", class_=\"header-bookmark-count js-header-bookmark-count\"\n",
    "                ).text\n",
    "            )\n",
    "        except:\n",
    "            restaurant_being_saved_times.append(\"N/A\")\n",
    "\n",
    "        try:\n",
    "            restaurant_district.append(\n",
    "                soup.find(\"div\", class_=\"header-poi-district dot-separator\").text\n",
    "            )\n",
    "        except:\n",
    "            restaurant_district.append(\"N/A\")\n",
    "\n",
    "        try:\n",
    "\n",
    "            restaurant_address.append(\n",
    "                [\n",
    "                    i.text\n",
    "                    for i in soup.find(\"div\", class_=\"address-info-section\").find_all(\n",
    "                        \"a\"\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        except:\n",
    "            restaurant_address.append(\"N/A\")\n",
    "\n",
    "        try:\n",
    "\n",
    "            restaurant_mtr_exit_or_mins_walk.append(\n",
    "                soup.find(\"section\", class_=\"transport-section\")\n",
    "                .find(\"div\", class_=\"content js-text-wrapper\")\n",
    "                .text\n",
    "            )\n",
    "        except:\n",
    "            restaurant_mtr_exit_or_mins_walk.append(\"N/A\")\n",
    "\n",
    "        try:\n",
    "\n",
    "            restaurant_branch_num.append(\n",
    "                soup.find(\"section\", class_=\"all-branches-section\").text\n",
    "            )\n",
    "        except:\n",
    "            restaurant_branch_num.append(\"N/A\")\n",
    "\n",
    "        try:\n",
    "\n",
    "            restaurant_opening_hours.append(\n",
    "                soup.find(\n",
    "                    \"section\", class_=\"opening-hours-container has-today-opening-hour\"\n",
    "                ).text\n",
    "            )\n",
    "        except:\n",
    "            restaurant_opening_hours.append(\"N/A\")\n",
    "\n",
    "        try:\n",
    "\n",
    "            restaurant_payment_method.append(\n",
    "                soup.find(\"div\", class_=\"or-section-expandable collapse in\")\n",
    "                .find(\"div\", class_=\"comma-tags\")\n",
    "                .text\n",
    "            )\n",
    "        except:\n",
    "            restaurant_payment_method.append(\"N/A\")\n",
    "\n",
    "    driver.quit()\n",
    "    print(f\"Scraped {scraped_num_restaurants} of restaurants\")\n",
    "\n",
    "print(\"----------------------------Done Scraping--------------------------------\")\n",
    "\n",
    "\n",
    "# Now put all the lists into a dataframe\n",
    "df_resto_info = pd.DataFrame(\n",
    "    {\n",
    "        \"scraping_status\": scraping_status,\n",
    "        \"restaurant_url\": restaurant_url,\n",
    "        \"restaurant_name\": restaurant_name,\n",
    "        \"restaurant_bookmarked_num\": restaurant_being_saved_times,\n",
    "        \"restaurant_district\": restaurant_district,\n",
    "        \"restaurant_address\": restaurant_address,\n",
    "        \"restaurant_mtr_exit_or_mins_walk\": restaurant_mtr_exit_or_mins_walk,\n",
    "        \"restaurant_branch_num\": restaurant_branch_num,\n",
    "        \"restaurant_opening_hours\": restaurant_opening_hours,\n",
    "        \"restaurant_payment_method\": restaurant_payment_method,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Convert the result DataFrame to a dictionary\n",
    "result_dict = df_resto_info.to_dict(orient=\"records\")\n",
    "\n",
    "# Convert the dictionary to a JSON string with indentation\n",
    "restaurant_info_json = json.dumps(result_dict, indent=4)\n",
    "\n",
    "# Write the JSON string to a file\n",
    "with open(\n",
    "    rf\"<Your_Path>\\OpenRice_Restaurant_Info_{today}.json\",\n",
    "    \"w\",\n",
    ") as file:\n",
    "    file.write(restaurant_info_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The following code is for getting first 5 pages of comment of each restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "from datetime import date, datetime\n",
    "\n",
    "import bs4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import stem.process\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from fake_useragent import UserAgent\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select, WebDriverWait\n",
    "\n",
    "today = str(date.today()).replace(\"-\", \"\")\n",
    "\n",
    "\n",
    "\"\"\"--------------------------------------------------------Setting Up TOR ----------------------------------------------------------------\"\"\"\n",
    "\n",
    "SOCKS_PORT = 9050\n",
    "TOR_PATH = os.path.normpath(r\"C:\\Program Files\\Tor-master\\tor\\tor.exe\")\n",
    "tor_process = stem.process.launch_tor_with_config(\n",
    "    config={\n",
    "        \"SocksPort\": str(SOCKS_PORT),\n",
    "        \"MaxCircuitDirtiness\": \"1\",\n",
    "    },\n",
    "    init_msg_handler=lambda line: (\n",
    "        print(line) if re.search(\"Bootstrapped\", line) else False\n",
    "    ),\n",
    "    tor_cmd=TOR_PATH,\n",
    ")\n",
    "\n",
    "PROXIES = {\"http\": \"socks5://127.0.0.1:9050\", \"https\": \"socks5://127.0.0.1:9050\"}\n",
    "\n",
    "url = \"http://ip-api.com/json/\"\n",
    "ua = UserAgent()\n",
    "header = ua.random\n",
    "headers = {\"User-Agent\": header}\n",
    "response = requests.get(url, proxies=PROXIES, headers=headers)\n",
    "result = json.loads(response.content)\n",
    "print(\n",
    "    \"TOR IP [%s]: %s %s\"\n",
    "    % (datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"), result[\"query\"], result[\"country\"])\n",
    ")\n",
    "\n",
    "# tor_process.kill()\n",
    "\n",
    "\"\"\"----------------------------------------------------------------------------------------------------------------------\"\"\"\n",
    "\n",
    "# soup_list = []\n",
    "scraping_status_list = []\n",
    "restaurant_comment_page_link_list = []\n",
    "account_grade_list = []\n",
    "account_link_list = []\n",
    "account_name_list = []\n",
    "comment_text_list = []\n",
    "taste_star_list = []\n",
    "env_star_list = []\n",
    "service_star_list = []\n",
    "hygiene_star_list = []\n",
    "fair_price_star_list = []\n",
    "dining_date_list = []\n",
    "dining_way_list = []\n",
    "price_per_person_list = []\n",
    "recommended_dishes_list = []\n",
    "dining_time_list = []\n",
    "restaurant_link_list = []\n",
    "restaurant_name_list = []\n",
    "restaurant_overall_rating = []\n",
    "restaurant_cuisine_type = []\n",
    "restaurant_price_range = []\n",
    "restaurant_smiley_smile = []\n",
    "restaurant_smiley_ok = []\n",
    "restaurant_smiley_cry = []\n",
    "comment_views_count_list = []\n",
    "\n",
    "df_restaurant_link = pd.read_json(rf\"<your address>\")\n",
    "\n",
    "\n",
    "def fetch_url_with_retries(url, max_retries=5, backoff_factor=1):\n",
    "    PROXIES = {\"http\": \"socks5://127.0.0.1:9050\", \"https\": \"socks5://127.0.0.1:9050\"}\n",
    "    ua = UserAgent()\n",
    "    header = ua.random\n",
    "    headers = {\"User-Agent\": header}\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, proxies=PROXIES, headers=headers, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except (ReadTimeout, ConnectionError) as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(backoff_factor * (2**attempt))\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "\n",
    "num_restro_scraped = 0\n",
    "link_prefix = \"https://www.openrice.com/\"\n",
    "for restaurant_link in list(df_restaurant_link[\"restaurant_link\"])[0:100]:\n",
    "    num_restro_scraped += 1\n",
    "    for page_num in range(1, 6):\n",
    "        resto_comment_page_link = (\n",
    "            str(restaurant_link) + r\"/reviews\" + rf\"?page={page_num}\"\n",
    "        )\n",
    "        try:\n",
    "\n",
    "            response = fetch_url_with_retries(resto_comment_page_link)\n",
    "            soup = bs4.BeautifulSoup(response.text, \"html.parser\")\n",
    "            restaurant_name = soup.find(\"h1\").find(\"span\", class_=\"name\").text\n",
    "\n",
    "        except:\n",
    "            account_name = \"N/A\"\n",
    "            account_grade = \"N/A\"\n",
    "            account_link = \"N/A\"\n",
    "            comment = \"N/A\"\n",
    "            comment_views_count = \"N/A\"\n",
    "            taste_star_num = \"N/A\"\n",
    "            env_star_num = \"N/A\"\n",
    "            service_star_num = \"N/A\"\n",
    "            hygiene_star_num = \"N/A\"\n",
    "            fair_price_star_num = \"N/A\"\n",
    "            dining_date = \"N/A\"\n",
    "            dining_way = \"N/A\"\n",
    "            price_per_person = \"N/A\"\n",
    "            recommended_dishes = \"N/A\"\n",
    "            dining_time = \"N/A\"\n",
    "            restaurant_name = \"N/A\"\n",
    "            resto_overall_rating = \"N/A\"\n",
    "            resto_cuisine_type = \"N/A\"\n",
    "            resto_price_range = \"N/A\"\n",
    "            smiley_smile = \"N/A\"\n",
    "            smiley_ok = \"N/A\"\n",
    "            smiley_cry = \"N/A\"\n",
    "            restaurant_comment_page_link_list.append(resto_comment_page_link)\n",
    "            account_name_list.append(account_name)\n",
    "            account_grade_list.append(account_grade)\n",
    "            account_link_list.append(account_link)\n",
    "            comment_text_list.append(comment)\n",
    "            comment_views_count_list.append(comment_views_count)\n",
    "            taste_star_list.append(taste_star_num)\n",
    "            env_star_list.append(env_star_num)\n",
    "            service_star_list.append(service_star_num)\n",
    "            hygiene_star_list.append(hygiene_star_num)\n",
    "            fair_price_star_list.append(fair_price_star_num)\n",
    "            dining_date_list.append(dining_date)\n",
    "            dining_way_list.append(dining_way)\n",
    "            price_per_person_list.append(price_per_person)\n",
    "            recommended_dishes_list.append(recommended_dishes)\n",
    "            dining_time_list.append(dining_time)\n",
    "            restaurant_link_list.append(restaurant_link)\n",
    "            restaurant_name_list.append(restaurant_name)\n",
    "            restaurant_overall_rating.append(resto_overall_rating)\n",
    "            restaurant_cuisine_type.append(resto_cuisine_type)\n",
    "            restaurant_price_range.append(resto_price_range)\n",
    "            restaurant_smiley_smile.append(smiley_smile)\n",
    "            restaurant_smiley_ok.append(smiley_ok)\n",
    "            restaurant_smiley_cry.append(smiley_cry)\n",
    "            scraping_status_list.append(\"Failed, need to re-scrape the page\")\n",
    "\n",
    "        else:\n",
    "\n",
    "            soup = bs4.BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            restaurant_name = soup.find(\"h1\").find(\"span\", class_=\"name\").text\n",
    "            if soup.find(\"div\", class_=\"header-score-show-more\"):\n",
    "                resto_overall_rating = soup.find(\n",
    "                    \"div\", class_=\"header-score-show-more\"\n",
    "                ).text\n",
    "\n",
    "            if soup.find(\"div\", class_=\"header-poi-price dot-separator\"):\n",
    "                resto_price_range = soup.find(\n",
    "                    \"div\", class_=\"header-poi-price dot-separator\"\n",
    "                ).text\n",
    "\n",
    "            if soup.find(\"div\", class_=\"header-poi-categories dot-separator\"):\n",
    "                resto_cuisine_type = soup.find(\n",
    "                    \"div\", class_=\"header-poi-categories dot-separator\"\n",
    "                ).text\n",
    "\n",
    "            if soup.find(\"div\", class_=\"header-smile-section\").find_all(\n",
    "                \"div\", class_=\"score-div\"\n",
    "            ):\n",
    "                smiley_smile = (\n",
    "                    soup.find(\"div\", class_=\"header-smile-section\")\n",
    "                    .find_all(\"div\", class_=\"score-div\")[0]\n",
    "                    .text\n",
    "                )\n",
    "                smiley_ok = (\n",
    "                    soup.find(\"div\", class_=\"header-smile-section\")\n",
    "                    .find_all(\"div\", class_=\"score-div\")[1]\n",
    "                    .text\n",
    "                )\n",
    "                smiley_cry = (\n",
    "                    soup.find(\"div\", class_=\"header-smile-section\")\n",
    "                    .find_all(\"div\", class_=\"score-div\")[2]\n",
    "                    .text\n",
    "                )\n",
    "\n",
    "            all_containers = soup.find_all(\n",
    "                \"div\",\n",
    "                class_=\"sr2-review-list-container full clearfix js-sr2-review-list-container\",\n",
    "            )\n",
    "\n",
    "            for contain in all_containers:\n",
    "\n",
    "                left_col = contain.find(\"div\", class_=\"left-col\")\n",
    "                account_name = left_col.find(\"div\", class_=\"name\").text\n",
    "                account_grade = left_col.find(\"div\", class_=\"grade-name\").text\n",
    "                account_link = link_prefix + left_col.find(\n",
    "                    \"a\", class_=\"or-user-avatar\"\n",
    "                ).get(\"href\")\n",
    "\n",
    "                comment = (\n",
    "                    contain.find(\n",
    "                        \"div\", class_=\"tab-pane content content-full js-content-full\"\n",
    "                    )\n",
    "                    .find(\"section\", class_=\"review-container\")\n",
    "                    .text\n",
    "                )\n",
    "                if contain.find(\"span\", class_=\"view-count\"):\n",
    "                    comment_views_count = contain.find(\"span\", class_=\"view-count\").text\n",
    "\n",
    "                right_col = contain.find(\"div\", class_=\"right-col\")\n",
    "\n",
    "                taste_star_num = \"N/A\"\n",
    "                env_star_num = \"N/A\"\n",
    "                service_star_num = \"N/A\"\n",
    "                hygiene_star_num = \"N/A\"\n",
    "                fair_price_star_num = \"N/A\"\n",
    "\n",
    "                # Iterate through the elements\n",
    "                for i in right_col.find_all(\"div\", class_=\"subject\"):\n",
    "                    if \"味道\" in i.text:\n",
    "                        taste_star_num = len(\n",
    "                            i.find_all(\n",
    "                                \"span\",\n",
    "                                class_=\"or-sprite-inline-block common_yellowstar_desktop\",\n",
    "                            )\n",
    "                        )\n",
    "                    if \"環境\" in i.text:\n",
    "                        env_star_num = len(\n",
    "                            i.find_all(\n",
    "                                \"span\",\n",
    "                                class_=\"or-sprite-inline-block common_yellowstar_desktop\",\n",
    "                            )\n",
    "                        )\n",
    "                    if \"服務\" in i.text:\n",
    "                        service_star_num = len(\n",
    "                            i.find_all(\n",
    "                                \"span\",\n",
    "                                class_=\"or-sprite-inline-block common_yellowstar_desktop\",\n",
    "                            )\n",
    "                        )\n",
    "                    if \"衛生\" in i.text:\n",
    "                        hygiene_star_num = len(\n",
    "                            i.find_all(\n",
    "                                \"span\",\n",
    "                                class_=\"or-sprite-inline-block common_yellowstar_desktop\",\n",
    "                            )\n",
    "                        )\n",
    "                    if \"抵食\" in i.text:\n",
    "                        fair_price_star_num = len(\n",
    "                            i.find_all(\n",
    "                                \"span\",\n",
    "                                class_=\"or-sprite-inline-block common_yellowstar_desktop\",\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                dining_date = \"N/A\"\n",
    "                dining_time = \"N/A\"\n",
    "                dining_way = \"N/A\"\n",
    "                price_per_person = \"N/A\"\n",
    "                recommended_dishes = \"N/A\"\n",
    "\n",
    "                # Iterate through the elements\n",
    "                other_dining_info = right_col.find_all(\n",
    "                    \"section\", class_=\"info-section detail\"\n",
    "                )\n",
    "                for i in other_dining_info:\n",
    "                    if \"用餐日期\" in i.text:\n",
    "                        dining_date_element = i.find(\"div\", class_=\"date text\")\n",
    "                        if dining_date_element:\n",
    "                            dining_date = dining_date_element.text\n",
    "                    if \"用餐時段\" in i.text:\n",
    "                        dining_time_element = i.find(\"div\", class_=\"price text\")\n",
    "                        if dining_time_element:\n",
    "                            dining_time = dining_time_element.text\n",
    "                    if \"用餐途徑\" in i.text:\n",
    "                        dining_way_element = i.find(\"div\", class_=\"text\")\n",
    "                        if dining_way_element:\n",
    "                            dining_way = dining_way_element.text\n",
    "                    if \"人均消費\" in i.text:\n",
    "                        price_per_person_element = i.find(\"div\", class_=\"price text\")\n",
    "                        if price_per_person_element:\n",
    "                            price_per_person = price_per_person_element.text\n",
    "                    if \"推介美食\" in i.text:\n",
    "                        recommended_dishes_element = i.find(\n",
    "                            \"div\", class_=\"list recommend-dish-name-list\"\n",
    "                        )\n",
    "                        if recommended_dishes_element:\n",
    "                            recommended_dishes = recommended_dishes_element.text\n",
    "\n",
    "                # Now append all the data to the respective lists\n",
    "                restaurant_comment_page_link_list.append(resto_comment_page_link)\n",
    "                account_name_list.append(account_name)\n",
    "                account_grade_list.append(account_grade)\n",
    "                account_link_list.append(account_link)\n",
    "                comment_text_list.append(comment)\n",
    "                comment_views_count_list.append(comment_views_count)\n",
    "                taste_star_list.append(taste_star_num)\n",
    "                env_star_list.append(env_star_num)\n",
    "                service_star_list.append(service_star_num)\n",
    "                hygiene_star_list.append(hygiene_star_num)\n",
    "                fair_price_star_list.append(fair_price_star_num)\n",
    "                dining_date_list.append(dining_date)\n",
    "                dining_way_list.append(dining_way)\n",
    "                price_per_person_list.append(price_per_person)\n",
    "                recommended_dishes_list.append(recommended_dishes)\n",
    "                dining_time_list.append(dining_time)\n",
    "                restaurant_link_list.append(restaurant_link)\n",
    "                restaurant_name_list.append(restaurant_name)\n",
    "                restaurant_overall_rating.append(resto_overall_rating)\n",
    "                restaurant_cuisine_type.append(resto_cuisine_type)\n",
    "                restaurant_price_range.append(resto_price_range)\n",
    "                restaurant_smiley_smile.append(smiley_smile)\n",
    "                restaurant_smiley_ok.append(smiley_ok)\n",
    "                restaurant_smiley_cry.append(smiley_cry)\n",
    "                scraping_status_list.append(\"Success\")\n",
    "\n",
    "            # soup_list.append(str(soup))\n",
    "        time.sleep(random.randint(3, 5))\n",
    "    print(f\"-------------Scraped {num_restro_scraped} restaurants-------------\")\n",
    "\n",
    "\n",
    "# Now put all the lists into a dataframe\n",
    "df_resto_comments = pd.DataFrame(\n",
    "    {\n",
    "        \"scraping_status\": scraping_status_list,\n",
    "        \"restaurant_name\": restaurant_name_list,\n",
    "        \"restaurant_link\": restaurant_link_list,\n",
    "        \"restaurant_comment_page_link\": restaurant_comment_page_link_list,\n",
    "        \"restaurant_overall_rating\": restaurant_overall_rating,\n",
    "        \"restaurant_cuisine_type\": restaurant_cuisine_type,\n",
    "        \"restaurant_price_range\": restaurant_price_range,\n",
    "        \"restaurant_smiley_smile\": restaurant_smiley_smile,\n",
    "        \"restaurant_smiley_ok\": restaurant_smiley_ok,\n",
    "        \"restaurant_smiley_cry\": restaurant_smiley_cry,\n",
    "        \"comment_account_name\": account_name_list,\n",
    "        \"comment_account_grade\": account_grade_list,\n",
    "        \"comment_account_link\": account_link_list,\n",
    "        \"comment_text\": comment_text_list,\n",
    "        \"comment_views_count\": comment_views_count_list,\n",
    "        \"comment_taste_star\": taste_star_list,\n",
    "        \"comment_env_star\": env_star_list,\n",
    "        \"comment_service_star\": service_star_list,\n",
    "        \"comment_hygiene_star\": hygiene_star_list,\n",
    "        \"comment_fair_price_star\": fair_price_star_list,\n",
    "        \"comment_dining_date\": dining_date_list,\n",
    "        \"comment_dining_time\": dining_time_list,\n",
    "        \"comment_dining_way\": dining_way_list,\n",
    "        \"comment_price_per_person\": price_per_person_list,\n",
    "        \"comment_recommended_dishes\": recommended_dishes_list,\n",
    "        # \"soup\": soup_list,\n",
    "    }\n",
    ")\n",
    "\n",
    "df_resto_comments.describe()\n",
    "\n",
    "# Convert the data type of column soup to string\n",
    "# df_resto_comments[\"soup\"] = df_resto_comments[\"soup\"].astype(str)\n",
    "\n",
    "# Convert the result DataFrame to a dictionary\n",
    "result_dict = df_resto_comments.to_dict(orient=\"records\")\n",
    "\n",
    "# Convert the dictionary to a JSON string with indentation\n",
    "restaurant_link_json = json.dumps(result_dict, indent=4)\n",
    "\n",
    "# Write the JSON string to a file\n",
    "with open(\n",
    "    rf\"<your address>.json\",\n",
    "    \"w\",\n",
    ") as file:\n",
    "    file.write(restaurant_link_json)\n",
    "\n",
    "print(\"----------------Done-------------------\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The following code is for getting the user information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "from datetime import date, datetime\n",
    "\n",
    "import bs4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import stem.process\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from fake_useragent import UserAgent\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select, WebDriverWait\n",
    "\n",
    "today = str(date.today()).replace(\"-\", \"\")\n",
    "\n",
    "\n",
    "\"\"\"--------------------------------------------------------Setting Up TOR ----------------------------------------------------------------\"\"\"\n",
    "\n",
    "SOCKS_PORT = 9050\n",
    "TOR_PATH = os.path.normpath(r\"C:\\Program Files\\Tor-master\\tor\\tor.exe\")\n",
    "tor_process = stem.process.launch_tor_with_config(\n",
    "    config={\n",
    "        \"SocksPort\": str(SOCKS_PORT),\n",
    "        \"MaxCircuitDirtiness\": \"1\",\n",
    "    },\n",
    "    init_msg_handler=lambda line: (\n",
    "        print(line) if re.search(\"Bootstrapped\", line) else False\n",
    "    ),\n",
    "    tor_cmd=TOR_PATH,\n",
    ")\n",
    "\n",
    "PROXIES = {\"http\": \"socks5://127.0.0.1:9050\", \"https\": \"socks5://127.0.0.1:9050\"}\n",
    "\n",
    "url = \"http://ip-api.com/json/\"\n",
    "ua = UserAgent()\n",
    "header = ua.random\n",
    "headers = {\"User-Agent\": header}\n",
    "response = requests.get(url, proxies=PROXIES, headers=headers)\n",
    "result = json.loads(response.content)\n",
    "print(\n",
    "    \"TOR IP [%s]: %s %s\"\n",
    "    % (datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"), result[\"query\"], result[\"country\"])\n",
    ")\n",
    "\n",
    "# tor_process.kill()\n",
    "\n",
    "\"\"\"----------------------------------------------------------------------------------------------------------------------\"\"\"\n",
    "\n",
    "# Import json as dataFrame\n",
    "df_comment_data = pd.read_json(\n",
    "    r\"C:\\Users\\ngaih\\OneDrive - The Chinese University of Hong Kong\\CUHK\\Courses\\STAT 5106 (Programming Techniques)\\Openrice\\OpenRice_Restaurant_data_20241112.json\"\n",
    ")\n",
    "\n",
    "df_excluded_failed = df_comment_data[\n",
    "    df_comment_data[\"scraping_status\"] != \"Failed, need to re-scrape the page\"\n",
    "].reset_index(drop=True)\n",
    "\n",
    "\n",
    "distinct_url_list_original_order = list(\n",
    "    df_excluded_failed[\"comment_account_link\"].unique()\n",
    ")\n",
    "\n",
    "len(distinct_url_list_original_order)\n",
    "\n",
    "# Now in distinct_url_list_original_order, I want to get one url every 200 urls\n",
    "every_50_urls = distinct_url_list_original_order[::50]\n",
    "len(every_50_urls)\n",
    "\n",
    "\n",
    "# df_top_1000_most_comment_ac = (\n",
    "#     df_excluded_failed.groupby([\"comment_account_name\", \"comment_account_link\"])\n",
    "#     .size()\n",
    "#     .sort_values(ascending=False)\n",
    "#     .head(1000)\n",
    "#     .reset_index(name=\"count\")\n",
    "# )\n",
    "\n",
    "# df_top_1000_most_comment_ac[\"user_id\"] = df_top_1000_most_comment_ac[\n",
    "#     \"comment_account_link\"\n",
    "# ].str.extract(r\"userid=(\\d+).?\")\n",
    "\n",
    "df_url_every_50_url = pd.DataFrame(every_50_urls, columns=[\"comment_account_link\"])\n",
    "\n",
    "df_url_every_50_url[\"user_id\"] = df_url_every_50_url[\n",
    "    \"comment_account_link\"\n",
    "].str.extract(r\"userid=(\\d+).?\")\n",
    "\n",
    "\n",
    "num_of_user_id = 0\n",
    "scraping_status = []\n",
    "member_name = []\n",
    "member_url = []\n",
    "member_level = []\n",
    "member_total_num_of_comment = []\n",
    "member_total_num_of_distinct_commented_resto = []\n",
    "member_total_num_of_uploaded_img_vid = []\n",
    "num_of_comment_in_this_url = []\n",
    "comment_header = []\n",
    "comment_restaurant = []\n",
    "comment_restaurant_url = []\n",
    "comment_over_rating_sticker = []\n",
    "comment_date = []\n",
    "comment_text = []\n",
    "taste_rating = []\n",
    "env_rating = []\n",
    "service_rating = []\n",
    "hygiene_rating = []\n",
    "fair_price_rating = []\n",
    "comment_individual_img_vid_and_its_caption = []\n",
    "total_num_of_comment = \"N/A\"\n",
    "total_num_of_distinct_commented_resto = \"N/A\"\n",
    "total_num_of_uploaded_img_vid = \"N/A\"\n",
    "openrice_url_prefix = \"https://www.openrice.com\"\n",
    "\n",
    "for index, row in df_url_every_50_url[750:1000].iterrows():\n",
    "\n",
    "    num_of_comment_url_in_user_ac = 0\n",
    "    num_of_user_id += 1\n",
    "\n",
    "    for num in range(1, 6):\n",
    "\n",
    "        num_of_comment_url_in_user_ac += 1\n",
    "\n",
    "        url = f\"https://www.openrice.com/zh/gourmet/reviews.htm?userid={row['user_id']}&city=hongkong&page={num}\"\n",
    "\n",
    "        PROXIES = \"socks5://127.0.0.1:9050\"\n",
    "\n",
    "        options = Options()\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_argument(f\"--proxy-server={PROXIES}\")\n",
    "\n",
    "        try:\n",
    "            # response = requests.get(url, headers=headers, proxies=PROXIES)\n",
    "            driver = webdriver.Chrome(options=options)\n",
    "            driver.get(url)\n",
    "            time.sleep(random.randint(3, 5))\n",
    "            # Now add code to randomly scroll the page and mimic human behavior\n",
    "            # url_height = random.randint(500, 1000)\n",
    "            url_height = random.choice(range(500, 1001, 50))\n",
    "            driver.execute_script(f\"window.scrollTo(0,{url_height})\")\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            driver.quit\n",
    "            soup.find(\"span\", id=\"user_username\").text\n",
    "            soup.find(\"div\", id=\"pagingHeader\").text.strip()\n",
    "\n",
    "        except:\n",
    "            scraping_status.append(\"Scraping Failed, need to rescrape this url\")\n",
    "            member_url.append(\"N/A\")\n",
    "            member_name.append(\"N/A\")\n",
    "            member_level.append(\"N/A\")\n",
    "            member_total_num_of_comment.append(\"N/A\")\n",
    "            member_total_num_of_distinct_commented_resto.append(\"N/A\")\n",
    "            member_total_num_of_uploaded_img_vid.append(\"N/A\")\n",
    "            num_of_comment_in_this_url.append(\"N/A\")\n",
    "            comment_header.append([\"N/A\"])\n",
    "            comment_restaurant.append([\"N/A\"])\n",
    "            comment_restaurant_url.append([\"N/A\"])\n",
    "            comment_over_rating_sticker.append([\"N/A\"])\n",
    "            comment_date.append([\"N/A\"])\n",
    "            comment_text.append([\"N/A\"])\n",
    "            taste_rating.append([\"N/A\"])\n",
    "            env_rating.append([\"N/A\"])\n",
    "            service_rating.append([\"N/A\"])\n",
    "            hygiene_rating.append([\"N/A\"])\n",
    "            fair_price_rating.append([\"N/A\"])\n",
    "            comment_individual_img_vid_and_its_caption.append([\"N/A\"])\n",
    "            time.sleep(random.randint(3, 5))\n",
    "\n",
    "        else:\n",
    "            # soup = bs4.BeautifulSoup(response.text, \"html.parser\")\n",
    "            scraping_status.append(\"Scraping Successful\")\n",
    "            member_url.append(url)\n",
    "            member_name.append(soup.find(\"span\", id=\"user_username\").text)\n",
    "            member_level.append(soup.find(\"div\", class_=\"txt_13 MT5\").text)\n",
    "            overview_tabs = soup.find_all(\"a\", class_=\"myor_menu_tab a_no_underline\")\n",
    "            num_of_comment_in_this_url.append(\n",
    "                soup.find(\"div\", id=\"pagingHeader\").text.strip()\n",
    "            )\n",
    "            for tab in overview_tabs:\n",
    "                if (\n",
    "                    tab.find(\"div\", class_=\"txt-small\")\n",
    "                    and \"食評\" in tab.find(\"div\", class_=\"txt-small\").text\n",
    "                ):\n",
    "                    count_div = tab.find(\"div\", class_=\"count\")\n",
    "                    total_num_of_comment = count_div.text.strip()\n",
    "            member_total_num_of_comment.append(total_num_of_comment)\n",
    "\n",
    "            for tab in overview_tabs:\n",
    "                if (\n",
    "                    tab.find(\"div\", class_=\"txt-small\")\n",
    "                    and \"我的餐廳\" in tab.find(\"div\", class_=\"txt-small\").text\n",
    "                ):\n",
    "                    count_div = tab.find(\"div\", class_=\"count\")\n",
    "                    total_num_of_distinct_commented_resto = count_div.text.strip()\n",
    "            member_total_num_of_distinct_commented_resto.append(\n",
    "                total_num_of_distinct_commented_resto\n",
    "            )\n",
    "\n",
    "            for tab in overview_tabs:\n",
    "                if (\n",
    "                    tab.find(\"div\", class_=\"txt-small\")\n",
    "                    and \"相片/影片\" in tab.find(\"div\", class_=\"txt-small\").text\n",
    "                ):\n",
    "                    count_div = tab.find(\"div\", class_=\"count\")\n",
    "                    total_num_of_uploaded_img_vid = count_div.text.strip()\n",
    "            member_total_num_of_uploaded_img_vid.append(total_num_of_uploaded_img_vid)\n",
    "\n",
    "            comment_header.append(\n",
    "                [\n",
    "                    title.text.strip() if title.text.strip() else \"N/A\"\n",
    "                    for title in soup.find_all(\"div\", class_=\"sr2_review_title\")\n",
    "                ]\n",
    "            )\n",
    "            comment_restaurant.append(\n",
    "                [\n",
    "                    restaurant.text.strip() if restaurant.text.strip() else \"N/A\"\n",
    "                    for restaurant in soup.find_all(\n",
    "                        \"div\", class_=\"pg_main txt_12 main_color2 MT5\"\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            comment_restaurant_url.append(\n",
    "                [\n",
    "                    (\n",
    "                        (openrice_url_prefix + restaurant.find(\"a\").get(\"href\"))\n",
    "                        if restaurant.find(\"a\").get(\"href\")\n",
    "                        else \"N/A\"\n",
    "                    )\n",
    "                    for restaurant in soup.find_all(\n",
    "                        \"div\", class_=\"pg_main txt_12 main_color2 MT5\"\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            comment_over_rating_sticker.append(\n",
    "                [\n",
    "                    overview.find(\"span\")[\"class\"] if overview.find(\"span\") else \"N/A\"\n",
    "                    for overview in soup.find_all(\"h1\", class_=\"rel_pos\")\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            comment_date.append(\n",
    "                [\n",
    "                    date.text.strip() if date.text.strip() else \"N/A\"\n",
    "                    for date in soup.find_all(\"div\", class_=\"FR MR5\")\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            comment_text.append(\n",
    "                [\n",
    "                    text.text.strip() if text.text.strip() else \"N/A\"\n",
    "                    for text in soup.find_all(\"div\", class_=\"sr2_review_full\")\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            taste_rating.append(\n",
    "                [\n",
    "                    5 - len(j.find_all(\"div\", class_=\"vis_hidden\"))\n",
    "                    for i in soup.find_all(\"div\", class_=\"FR\", style=\"width:300px\")\n",
    "                    for j in i.find_all(\"div\", class_=\"FL\")\n",
    "                    if \"味道\" in j.text\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            env_rating.append(\n",
    "                [\n",
    "                    5 - len(j.find_all(\"div\", class_=\"vis_hidden\"))\n",
    "                    for i in soup.find_all(\"div\", class_=\"FR\", style=\"width:300px\")\n",
    "                    for j in i.find_all(\"div\", class_=\"FL\")\n",
    "                    if \"環境\" in j.text\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            service_rating.append(\n",
    "                [\n",
    "                    5 - len(j.find_all(\"div\", class_=\"vis_hidden\"))\n",
    "                    for i in soup.find_all(\"div\", class_=\"FR\", style=\"width:300px\")\n",
    "                    for j in i.find_all(\"div\", class_=\"FL\")\n",
    "                    if \"服務\" in j.text\n",
    "                ]\n",
    "            )\n",
    "            hygiene_rating.append(\n",
    "                [\n",
    "                    5 - len(j.find_all(\"div\", class_=\"vis_hidden\"))\n",
    "                    for i in soup.find_all(\"div\", class_=\"FR\", style=\"width:300px\")\n",
    "                    for j in i.find_all(\"div\", class_=\"FL\")\n",
    "                    if \"衛生\" in j.text\n",
    "                ]\n",
    "            )\n",
    "            fair_price_rating.append(\n",
    "                [\n",
    "                    5 - len(j.find_all(\"div\", class_=\"vis_hidden\"))\n",
    "                    for i in soup.find_all(\"div\", class_=\"FR\", style=\"width:300px\")\n",
    "                    for j in i.find_all(\"div\", class_=\"FL\")\n",
    "                    if \"抵食\" in j.text\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            comment_individual_img_vid_and_its_caption.append(\n",
    "                [\n",
    "                    len(i.find_all(\"img\", class_=\"lazyload review_img_og\"))\n",
    "                    for i in soup.find_all(\"div\", class_=\"sr2_review_full\")\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            time.sleep(random.randint(3, 5))\n",
    "\n",
    "            print(\n",
    "                f\"---------------------Scraping {num_of_user_id} user_account and {num_of_comment_url_in_user_ac} comment page in this account -----------------------------\"\n",
    "            )\n",
    "\n",
    "print(\"-----------------------Done-----------------------------\")\n",
    "\n",
    "# Now create a dataframe to store the results\n",
    "df_final_result = pd.DataFrame(\n",
    "    {\n",
    "        \"scraping_status\": scraping_status,\n",
    "        \"member_name\": member_name,\n",
    "        \"member_url\": member_url,\n",
    "        \"member_level\": member_level,\n",
    "        \"member_total_num_of_comment\": member_total_num_of_comment,\n",
    "        \"member_total_num_of_distinct_commented_resto\": member_total_num_of_distinct_commented_resto,\n",
    "        \"member_total_num_of_uploaded_img_vid\": member_total_num_of_uploaded_img_vid,\n",
    "        \"num_of_comment_in_this_url\": num_of_comment_in_this_url,\n",
    "        \"comment_header\": comment_header,\n",
    "        \"comment_restaurant\": comment_restaurant,\n",
    "        \"comment_restaurant_url\": comment_restaurant_url,\n",
    "        \"comment_over_rating_sticker\": comment_over_rating_sticker,\n",
    "        \"comment_date\": comment_date,\n",
    "        \"comment_text\": comment_text,\n",
    "        \"taste_rating\": taste_rating,\n",
    "        \"env_rating\": env_rating,\n",
    "        \"service_rating\": service_rating,\n",
    "        \"hygiene_rating\": hygiene_rating,\n",
    "        \"fair_price_rating\": fair_price_rating,\n",
    "        \"individual_comment_img_vid_and_its_caption\": comment_individual_img_vid_and_its_caption,\n",
    "    }\n",
    ")\n",
    "\n",
    "result_dict = df_final_result.to_dict(orient=\"records\")\n",
    "\n",
    "# Convert the dictionary to a JSON string with indentation\n",
    "user_info = json.dumps(result_dict, indent=4)\n",
    "\n",
    "# Write the JSON string to a file\n",
    "with open(\n",
    "    rf\"C:\\Users\\ngaih\\OneDrive - The Chinese University of Hong Kong\\CUHK\\Courses\\STAT 5106 (Programming Techniques)\\Openrice\\User_Info_{today}.json\",\n",
    "    \"w\",\n",
    ") as file:\n",
    "    file.write(user_info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. The following code is converting the restaurant Chinese Address to Latitude and Longitude for plotting map graph in tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from components.core import Address\n",
    "from components.util import Similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "# List of JSON file paths\n",
    "json_files = [\n",
    "    r\"C:\\Users\\ngaih\\OneDrive - The Chinese University of Hong Kong\\CUHK\\Courses\\STAT 5106 (Programming Techniques)\\Openrice\\Restaurant_Info\\OneDrive_1_12-12-2024\\OpenRice_Restaurant_Info_20241211[5000,5500].json\",\n",
    "    r\"C:\\Users\\ngaih\\OneDrive - The Chinese University of Hong Kong\\CUHK\\Courses\\STAT 5106 (Programming Techniques)\\Openrice\\Restaurant_Info\\OneDrive_1_12-12-2024\\OpenRice_Restaurant_Info_0_to_800_20241207.json\",\n",
    "    r\"C:\\Users\\ngaih\\OneDrive - The Chinese University of Hong Kong\\CUHK\\Courses\\STAT 5106 (Programming Techniques)\\Openrice\\Restaurant_Info\\OneDrive_1_12-12-2024\\OpenRice_Restaurant_Info_800_to_1600_20241208.json\",\n",
    "    r\"C:\\Users\\ngaih\\OneDrive - The Chinese University of Hong Kong\\CUHK\\Courses\\STAT 5106 (Programming Techniques)\\Openrice\\Restaurant_Info\\OneDrive_1_12-12-2024\\OpenRice_Restaurant_Info_1600_to_2400_20241208.json\",\n",
    "    r\"C:\\Users\\ngaih\\OneDrive - The Chinese University of Hong Kong\\CUHK\\Courses\\STAT 5106 (Programming Techniques)\\Openrice\\Restaurant_Info\\OneDrive_1_12-12-2024\\OpenRice_Restaurant_Info_2400_to_3000_20241208.json\",\n",
    "    r\"C:\\Users\\ngaih\\OneDrive - The Chinese University of Hong Kong\\CUHK\\Courses\\STAT 5106 (Programming Techniques)\\Openrice\\Restaurant_Info\\OneDrive_1_12-12-2024\\OpenRice_Restaurant_Info_3000_to_3800_20241209.json\",\n",
    "    r\"C:\\Users\\ngaih\\OneDrive - The Chinese University of Hong Kong\\CUHK\\Courses\\STAT 5106 (Programming Techniques)\\Openrice\\Restaurant_Info\\OneDrive_1_12-12-2024\\OpenRice_Restaurant_Info_3800_to_5000_20241210.json\",\n",
    "    r\"C:\\Users\\ngaih\\OneDrive - The Chinese University of Hong Kong\\CUHK\\Courses\\STAT 5106 (Programming Techniques)\\Openrice\\Restaurant_Info\\OneDrive_1_12-12-2024\\OpenRice_Restaurant_Info_20241203 [8500,9006].json\",\n",
    "    r\"C:\\Users\\ngaih\\OneDrive - The Chinese University of Hong Kong\\CUHK\\Courses\\STAT 5106 (Programming Techniques)\\Openrice\\Restaurant_Info\\OneDrive_1_12-12-2024\\OpenRice_Restaurant_Info_20241204 [7000,8000].json\",\n",
    "    r\"C:\\Users\\ngaih\\OneDrive - The Chinese University of Hong Kong\\CUHK\\Courses\\STAT 5106 (Programming Techniques)\\Openrice\\Restaurant_Info\\OneDrive_1_12-12-2024\\OpenRice_Restaurant_Info_20241204 [8000,8500].json\",\n",
    "    r\"C:\\Users\\ngaih\\OneDrive - The Chinese University of Hong Kong\\CUHK\\Courses\\STAT 5106 (Programming Techniques)\\Openrice\\Restaurant_Info\\OneDrive_1_12-12-2024\\OpenRice_Restaurant_Info_20241205 [6500,7000].json\",\n",
    "    r\"C:\\Users\\ngaih\\OneDrive - The Chinese University of Hong Kong\\CUHK\\Courses\\STAT 5106 (Programming Techniques)\\Openrice\\Restaurant_Info\\OneDrive_1_12-12-2024\\OpenRice_Restaurant_Info_20241209 [6450,6500].json\",\n",
    "    r\"C:\\Users\\ngaih\\OneDrive - The Chinese University of Hong Kong\\CUHK\\Courses\\STAT 5106 (Programming Techniques)\\Openrice\\Restaurant_Info\\OneDrive_1_12-12-2024\\OpenRice_Restaurant_Info_20241209[6400,6450].json\",\n",
    "    r\"C:\\Users\\ngaih\\OneDrive - The Chinese University of Hong Kong\\CUHK\\Courses\\STAT 5106 (Programming Techniques)\\Openrice\\Restaurant_Info\\OneDrive_1_12-12-2024\\OpenRice_Restaurant_Info_20241210[5500,6400].json\",\n",
    "]\n",
    "\n",
    "# Read each JSON file into a DataFrame and store them in a list\n",
    "dataframes = [pd.read_json(file) for file in json_files]\n",
    "\n",
    "# Concatenate all DataFrames vertically\n",
    "df = pd.concat(dataframes, axis=0)\n",
    "\n",
    "# Reset the index of the concatenated DataFrame\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "df = df[\n",
    "    df[\"scraping_status\"] != \"Failed to scrape, need to rescrape the url\"\n",
    "].reset_index(drop=True)\n",
    "\n",
    "df[\"restaurant_address\"] = (\n",
    "    df[\"restaurant_address\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\"\\\\n\", \"\")\n",
    "    .str.replace(\",\", \"\")\n",
    "    .str.replace(\"'\", \"\")\n",
    "    .str.replace(\" \", \"\")\n",
    "    .str.replace(\"[\", \"\")\n",
    "    .str.replace(\"]\", \"\")\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "address_list = df[\"restaurant_address\"].tolist()\n",
    "len(address_list)\n",
    "\n",
    "lat = []\n",
    "lon = []\n",
    "region = []\n",
    "district = []\n",
    "street = []\n",
    "estate = []\n",
    "num = 0\n",
    "for i in address_list[0:]:\n",
    "    num += 1\n",
    "    address = Address(i)\n",
    "    result = address.ParseAddress()\n",
    "    lat.append(result[\"geo\"].get(\"Latitude\"))\n",
    "    lon.append(result[\"geo\"].get(\"Longitude\"))\n",
    "    region.append(result.get(\"chi\", \"N/A\").get(\"Region\", \"N/A\"))\n",
    "    district.append(result.get(\"chi\", \"N/A\").get(\"ChiDistrict\", \"N/A\"))\n",
    "    street.append(result.get(\"chi\", \"N/A\").get(\"ChiStreet\", \"N/A\"))\n",
    "    estate.append(result.get(\"chi\", \"N/A\").get(\"ChiEstate\", \"N/A\"))\n",
    "    if num % 200 == 0:\n",
    "        print(num)\n",
    "        time.sleep(1)\n",
    "\n",
    "df[\"Latitude\"] = lat\n",
    "df[\"Longitude\"] = lon\n",
    "df[\"Region\"] = region\n",
    "df[\"District\"] = district\n",
    "df[\"Street\"] = street\n",
    "df[\"Estate\"] = estate\n",
    "\n",
    "# get StreetName from element of street\n",
    "df[\"Exact_Street\"] = df[\"Street\"].apply(\n",
    "    lambda x: x.get(\"StreetName\", \"N/A\") if x != \"N/A\" else \"N/A\"\n",
    ")\n",
    "df[\"Exact_District\"] = df[\"District\"].apply(\n",
    "    lambda x: x.get(\"DcDistrict\", \"N/A\") if x != \"N/A\" else \"N/A\"\n",
    ")\n",
    "\n",
    "\n",
    "df_resto_lat_lon = df[\n",
    "    [\n",
    "        \"restaurant_name\",\n",
    "        \"restaurant_url\",\n",
    "        \"restaurant_address\",\n",
    "        \"Latitude\",\n",
    "        \"Longitude\",\n",
    "        \"Region\",\n",
    "        \"District\",\n",
    "        \"Street\",\n",
    "        \"Estate\",\n",
    "        \"StreetName\",\n",
    "        \"Exact_District\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "df_resto_lat_lon.to_csv(\n",
    "    r\"C:\\Users\\ngaih\\OneDrive - The Chinese University of Hong Kong\\CUHK\\Courses\\STAT 5106 (Programming Techniques)\\Openrice\\Restaurant_Info\\resto_lat_lon_20241212.csv\",\n",
    "    sep=\";\",\n",
    "    index=False,\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
